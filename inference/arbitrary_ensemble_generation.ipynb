{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2611231c",
   "metadata": {},
   "source": [
    "This notebook illustrates how to generate an ensemble of model predictions by arbitrarily sampling the latent space of electrophysiological features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a54162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.input_builder import build_input, build_input_with_embeddings, extract_scaled_e_features\n",
    "from utils.model_utils import load_model\n",
    "from utils.latent_sampler import sampled_2d_latent_space\n",
    "from utils.plotting import plot_ensemble_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4001c",
   "metadata": {},
   "source": [
    "**Step 1: Set up parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c628448",
   "metadata": {},
   "outputs": [],
   "source": [
    "device      = \"cpu\"\n",
    "amplitude   = 0.5\n",
    "num_samples = 200\n",
    "\n",
    "dt_original       = 0.02\n",
    "downsample_factor = 3\n",
    "\n",
    "features_to_embed = [\"slope\", \"intercept\"]\n",
    "\n",
    "embedding_config = {\"num_current_embeddings\": 9,\n",
    "                    \"type_current_embeddings\": \"freq\",\n",
    "                    \"num_hof_model_embeddings\": 1,\n",
    "                    \"type_hof_model_embeddings\": \"freq\",\n",
    "                    \"e_features_to_embed\": features_to_embed}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbadf5e",
   "metadata": {},
   "source": [
    "**Step 2: Load electrophysiological features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_features = extract_scaled_e_features(neuron_identifier=\"PVALB_689331391\", \n",
    "                                                path_to_features='../data/e_features/pvalb_689331391_ephys_sim_features.csv', \n",
    "                                                features_to_embed=features_to_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0bc21f",
   "metadata": {},
   "source": [
    "**Step 3: Load trained $\\texttt{NOBLE}$ model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a43d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'noble_models/FNO_nmodes-256_in-23_out-1_nlayers-12_projectionratio-4_hc-24_AmpEmbeddings-FreqScaledNeRF-nfreq-9_HoFEmbeddings-FreqScaledNeRF-nfreq-1_bestepoch-296.pth'\n",
    "model = load_model(model_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5648c",
   "metadata": {},
   "source": [
    "**Step 4: Sample latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510b9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_hof_models     = np.arange(0, 60)\n",
    "hof_test             = np.array([32, 2, 12, 40, 5, 52, 21, 29, 16, 37])\n",
    "hof_train            = np.setdiff1d(known_hof_models, hof_test)\n",
    "features_train       = normalised_features[normalised_features[\"hof_model\"].isin(hof_train)]\n",
    "\n",
    "latent_samples = sampled_2d_latent_space(normalised_features_train=features_train, \n",
    "                                         features_to_embed=features_to_embed,\n",
    "                                         num_samples=num_samples)\n",
    "\n",
    "sampled_models = {\"intercept\": torch.tensor(latent_samples[:, 0], dtype=torch.float32), \n",
    "                  \"slope\": torch.tensor(latent_samples[:, 1], dtype=torch.float32)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24304bf7",
   "metadata": {},
   "source": [
    "**Step 5: Build input with sampled model embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch_transformed = build_input_with_embeddings(input_batch=build_input(amplitude, num_samples, device), \n",
    "                                                      embedding_config=embedding_config, \n",
    "                                                      features_to_embed=features_to_embed, \n",
    "                                                      normalised_features=normalised_features, \n",
    "                                                      device=device,\n",
    "                                                      sampled_embeddings=sampled_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea52180",
   "metadata": {},
   "source": [
    "**Step 6: Generate ensemble predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbdc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_batch = model(input_batch_transformed)\n",
    "\n",
    "plot_ensemble_prediction(predicted_output = output_batch.squeeze(1).cpu().numpy(), \n",
    "                         num_samples = num_samples, \n",
    "                         dt_downsampled = dt_original * downsample_factor, \n",
    "                         save_title=\"arbitrary_ensemble_prediction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
